{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas基礎② - データクリーニング（通常版）\n",
    "\n",
    "Day 4 - Pandas Data Cleaning Basic\n",
    "\n",
    "実務で必要なデータクリーニング技術を体系的に学習します。\n",
    "より高度な処理や、大規模データへの対応方法も含みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\"=== Pandas データクリーニング基礎（通常版） ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 複雑なサンプルデータの作成\n",
    "\n",
    "実務でよく遭遇する様々な問題を含んだ販売トランザクションデータを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 販売トランザクションデータ（実務でよくある問題を含む）\n",
    "np.random.seed(42)\n",
    "n_records = 100\n",
    "\n",
    "data = {\n",
    "    'transaction_id': [f'T{str(i).zfill(4)}' for i in range(1, n_records + 1)],\n",
    "    'date': pd.date_range('2024-01-01', periods=n_records, freq='h').astype(str),\n",
    "    'customer_id': np.random.choice(['C001', 'C002', 'C003', None, 'C004', 'C005'], n_records),\n",
    "    'product_name': np.random.choice(['Product A', 'Product B', ' Product C ', 'product a', None], n_records),\n",
    "    'quantity': np.random.choice([1, 2, 3, '4', '5個', None, -1], n_records),\n",
    "    'unit_price': np.random.choice([1000, 1500, '2,000', '¥3000', None], n_records),\n",
    "    'category': np.random.choice(['Electronics', 'electronics', 'ELECTRONICS', 'Clothing', None], n_records),\n",
    "    'status': np.random.choice(['completed', 'Completed', 'COMPLETED', 'cancelled', 'pending'], n_records)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 意図的にいくつかの問題を追加\n",
    "df.loc[10:15, 'date'] = 'invalid_date'\n",
    "df.loc[20:25, 'quantity'] = '在庫切れ'\n",
    "df.loc[30:32, 'transaction_id'] = 'T0001'  # 重複ID\n",
    "\n",
    "print(\"データの先頭10行:\")\n",
    "print(df.head(10))\n",
    "print(f\"\\nデータの形状: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. データ品質の総合チェック\n",
    "\n",
    "データの品質を包括的にチェックする関数を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_quality_report(df):\n",
    "    \"\"\"データ品質レポートを生成する関数\"\"\"\n",
    "    report = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_info = {\n",
    "            'column': col,\n",
    "            'dtype': str(df[col].dtype),\n",
    "            'nulls': df[col].isnull().sum(),\n",
    "            'null_pct': f\"{df[col].isnull().sum() / len(df) * 100:.1f}%\",\n",
    "            'unique': df[col].nunique(),\n",
    "            'duplicates': len(df) - df[col].nunique() if df[col].dtype == 'object' else '-'\n",
    "        }\n",
    "        report.append(col_info)\n",
    "    \n",
    "    return pd.DataFrame(report)\n",
    "\n",
    "quality_report = data_quality_report(df)\n",
    "print(\"データ品質レポート:\")\n",
    "quality_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 高度な欠損値処理\n",
    "\n",
    "欠損値のパターンを分析し、適切な処理方法を選択します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 欠損値のパターン分析\n",
    "print(\"欠損値のパターン分析:\")\n",
    "missing_pattern = df.isnull().value_counts()\n",
    "print(missing_pattern.head())\n",
    "print()\n",
    "\n",
    "# 条件付き欠損値処理\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# カテゴリ変数の欠損値処理\n",
    "df_cleaned['category'] = df_cleaned['category'].fillna('Unknown')\n",
    "df_cleaned['product_name'] = df_cleaned['product_name'].fillna('Unknown Product')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数値変数の欠損値処理（グループ別平均値で補完）\n",
    "# まず数値に変換可能なものを変換\n",
    "df_cleaned['quantity'] = pd.to_numeric(df_cleaned['quantity'], errors='coerce')\n",
    "df_cleaned['unit_price'] = df_cleaned['unit_price'].astype(str).str.replace('[¥,円]', '', regex=True)\n",
    "df_cleaned['unit_price'] = pd.to_numeric(df_cleaned['unit_price'], errors='coerce')\n",
    "\n",
    "# カテゴリ別の平均値で補完\n",
    "for category in df_cleaned['category'].unique():\n",
    "    mask = df_cleaned['category'] == category\n",
    "    df_cleaned.loc[mask, 'unit_price'] = df_cleaned.loc[mask, 'unit_price'].fillna(\n",
    "        df_cleaned.loc[mask, 'unit_price'].mean()\n",
    "    )\n",
    "\n",
    "print(\"欠損値処理後の状況:\")\n",
    "print(df_cleaned.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. データ型の変換と検証\n",
    "\n",
    "各列を適切なデータ型に変換し、無効なデータを処理します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日付型への変換（エラー処理付き）\n",
    "df_cleaned['date'] = pd.to_datetime(df_cleaned['date'], errors='coerce')\n",
    "\n",
    "# 無効な日付の確認\n",
    "invalid_dates = df_cleaned[df_cleaned['date'].isnull()]['transaction_id']\n",
    "if len(invalid_dates) > 0:\n",
    "    print(f\"無効な日付を持つトランザクション: {list(invalid_dates.head())}\")\n",
    "    # 無効な日付を現在時刻で置換\n",
    "    df_cleaned['date'] = df_cleaned['date'].fillna(pd.Timestamp.now())\n",
    "\n",
    "# データ型の確認\n",
    "print(\"\\n変換後のデータ型:\")\n",
    "print(df_cleaned.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 文字列の正規化と標準化\n",
    "\n",
    "文字列データの表記揺れを統一します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前後の空白削除\n",
    "df_cleaned['product_name'] = df_cleaned['product_name'].str.strip()\n",
    "\n",
    "# 大文字小文字の統一\n",
    "df_cleaned['category'] = df_cleaned['category'].str.capitalize()\n",
    "df_cleaned['status'] = df_cleaned['status'].str.lower()\n",
    "\n",
    "# 製品名の統一（類似名称の統合）\n",
    "product_mapping = {\n",
    "    'product a': 'Product A',\n",
    "    'Product a': 'Product A',\n",
    "    'PRODUCT A': 'Product A'\n",
    "}\n",
    "df_cleaned['product_name'] = df_cleaned['product_name'].str.capitalize().replace(product_mapping)\n",
    "\n",
    "print(\"正規化後の値の分布:\")\n",
    "print(\"カテゴリ:\", df_cleaned['category'].value_counts().to_dict())\n",
    "print(\"ステータス:\", df_cleaned['status'].value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 外れ値の検出と処理\n",
    "\n",
    "IQR（四分位範囲）を使って外れ値を検出します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_iqr(df, column):\n",
    "    \"\"\"IQRを使った外れ値検出\"\"\"\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# 数量の外れ値検出\n",
    "quantity_outliers, q_lower, q_upper = detect_outliers_iqr(df_cleaned[df_cleaned['quantity'].notna()], 'quantity')\n",
    "print(f\"数量の外れ値: {len(quantity_outliers)}件\")\n",
    "print(f\"正常範囲: {q_lower:.1f} ～ {q_upper:.1f}\")\n",
    "\n",
    "# 負の数量を0に修正\n",
    "df_cleaned.loc[df_cleaned['quantity'] < 0, 'quantity'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 重複データの高度な処理\n",
    "\n",
    "完全重複と部分重複を区別して処理します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完全重複の確認\n",
    "full_duplicates = df_cleaned.duplicated().sum()\n",
    "print(f\"完全重複行数: {full_duplicates}\")\n",
    "\n",
    "# 部分重複の確認（transaction_idの重複）\n",
    "id_duplicates = df_cleaned[df_cleaned.duplicated(subset=['transaction_id'], keep=False)]\n",
    "print(f\"\\ntransaction_idの重複: {len(id_duplicates)}件\")\n",
    "\n",
    "if len(id_duplicates) > 0:\n",
    "    print(\"\\n重複したtransaction_id:\")\n",
    "    print(id_duplicates[['transaction_id', 'date', 'customer_id']].head())\n",
    "    \n",
    "    # 最新の日付のレコードを残す\n",
    "    df_cleaned = df_cleaned.sort_values('date').drop_duplicates(subset=['transaction_id'], keep='last')\n",
    "    print(f\"\\n重複削除後のレコード数: {len(df_cleaned)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. データ品質の最終検証\n",
    "\n",
    "クリーニング後のデータ品質を評価します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# クリーニング前後の比較\n",
    "print(\"クリーニング前後の比較:\")\n",
    "print(f\"元のレコード数: {len(df)}\")\n",
    "print(f\"クリーニング後のレコード数: {len(df_cleaned)}\")\n",
    "print(f\"削除されたレコード数: {len(df) - len(df_cleaned)}\")\n",
    "print(f\"欠損値の総数: {df_cleaned.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ品質スコアの計算\n",
    "def calculate_quality_score(df):\n",
    "    \"\"\"データ品質スコアを計算（0-100）\"\"\"\n",
    "    total_cells = df.size\n",
    "    null_cells = df.isnull().sum().sum()\n",
    "    \n",
    "    # 数値列の異常値チェック\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    outlier_count = 0\n",
    "    for col in numeric_cols:\n",
    "        if len(df[col].dropna()) > 0:\n",
    "            outliers, _, _ = detect_outliers_iqr(df[df[col].notna()], col)\n",
    "            outlier_count += len(outliers)\n",
    "    \n",
    "    # スコア計算\n",
    "    null_score = (1 - null_cells / total_cells) * 50\n",
    "    outlier_score = (1 - outlier_count / len(df)) * 30\n",
    "    duplicate_score = (1 - df.duplicated().sum() / len(df)) * 20\n",
    "    \n",
    "    total_score = null_score + outlier_score + duplicate_score\n",
    "    return total_score\n",
    "\n",
    "quality_score = calculate_quality_score(df_cleaned)\n",
    "print(f\"\\nデータ品質スコア: {quality_score:.1f}/100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. クリーニングされたデータの保存\n",
    "\n",
    "複数の形式でデータを保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSVファイルとして保存\n",
    "output_file = 'cleaned_transaction_data.csv'\n",
    "df_cleaned.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "print(f\"クリーニング済みデータを {output_file} として保存しました。\")\n",
    "\n",
    "# パーケット形式でも保存（大規模データに適している）\n",
    "df_cleaned.to_parquet('cleaned_transaction_data.parquet', index=False)\n",
    "print(\"パーケット形式でも保存しました（大規模データ用）。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 実践的なクリーニング関数\n",
    "\n",
    "再利用可能な包括的クリーニング関数を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_data_cleaning(df, config=None):\n",
    "    \"\"\"\n",
    "    包括的なデータクリーニング関数\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        クリーニング対象のデータフレーム\n",
    "    config : dict\n",
    "        クリーニング設定（オプション）\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    cleaned_df : pandas.DataFrame\n",
    "        クリーニング済みデータフレーム\n",
    "    report : dict\n",
    "        クリーニングレポート\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = {\n",
    "            'remove_duplicates': True,\n",
    "            'handle_missing': True,\n",
    "            'standardize_text': True,\n",
    "            'remove_outliers': False,\n",
    "            'date_columns': [],\n",
    "            'numeric_columns': [],\n",
    "            'categorical_columns': []\n",
    "        }\n",
    "    \n",
    "    cleaned_df = df.copy()\n",
    "    report = {\n",
    "        'original_rows': len(df),\n",
    "        'original_columns': len(df.columns),\n",
    "        'steps_performed': []\n",
    "    }\n",
    "    \n",
    "    # 1. 重複削除\n",
    "    if config['remove_duplicates']:\n",
    "        before_rows = len(cleaned_df)\n",
    "        cleaned_df = cleaned_df.drop_duplicates()\n",
    "        removed = before_rows - len(cleaned_df)\n",
    "        report['steps_performed'].append(f'重複削除: {removed}行削除')\n",
    "    \n",
    "    # 2. 欠損値処理\n",
    "    if config['handle_missing']:\n",
    "        missing_before = cleaned_df.isnull().sum().sum()\n",
    "        \n",
    "        # カテゴリ変数\n",
    "        for col in config.get('categorical_columns', []):\n",
    "            if col in cleaned_df.columns:\n",
    "                cleaned_df[col] = cleaned_df[col].fillna('Unknown')\n",
    "        \n",
    "        # 数値変数\n",
    "        for col in config.get('numeric_columns', []):\n",
    "            if col in cleaned_df.columns:\n",
    "                cleaned_df[col] = cleaned_df[col].fillna(cleaned_df[col].median())\n",
    "        \n",
    "        missing_after = cleaned_df.isnull().sum().sum()\n",
    "        report['steps_performed'].append(f'欠損値処理: {missing_before - missing_after}個処理')\n",
    "    \n",
    "    # 3. テキスト標準化\n",
    "    if config['standardize_text']:\n",
    "        text_cols = cleaned_df.select_dtypes(include=['object']).columns\n",
    "        for col in text_cols:\n",
    "            cleaned_df[col] = cleaned_df[col].astype(str).str.strip()\n",
    "        report['steps_performed'].append(f'テキスト標準化: {len(text_cols)}列処理')\n",
    "    \n",
    "    # 4. 日付変換\n",
    "    for col in config.get('date_columns', []):\n",
    "        if col in cleaned_df.columns:\n",
    "            cleaned_df[col] = pd.to_datetime(cleaned_df[col], errors='coerce')\n",
    "    \n",
    "    report['final_rows'] = len(cleaned_df)\n",
    "    report['rows_removed'] = report['original_rows'] - report['final_rows']\n",
    "    \n",
    "    return cleaned_df, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用例\n",
    "print(\"包括的クリーニング関数の使用例:\")\n",
    "config = {\n",
    "    'remove_duplicates': True,\n",
    "    'handle_missing': True,\n",
    "    'standardize_text': True,\n",
    "    'categorical_columns': ['category', 'status'],\n",
    "    'numeric_columns': ['quantity', 'unit_price'],\n",
    "    'date_columns': ['date']\n",
    "}\n",
    "\n",
    "# 新しいサンプルデータでテスト\n",
    "test_df = df.head(20).copy()\n",
    "cleaned_test, cleaning_report = comprehensive_data_cleaning(test_df, config)\n",
    "\n",
    "print(\"\\nクリーニングレポート:\")\n",
    "for key, value in cleaning_report.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "データクリーニングの重要ポイントを整理します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== まとめ ===\")\n",
    "print(\"データクリーニングの重要ポイント:\")\n",
    "print(\"1. 体系的なアプローチ: 品質チェック → 問題特定 → 処理 → 検証\")\n",
    "print(\"2. データの特性理解: 各列の型、分布、ビジネス意味を理解\")\n",
    "print(\"3. 処理の記録: 何を、なぜ、どのように処理したかを記録\")\n",
    "print(\"4. 自動化: 繰り返し使える関数やパイプラインの構築\")\n",
    "print(\"5. 検証: クリーニング後のデータ品質を必ず確認\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実践課題\n",
    "\n",
    "自分のデータでクリーニングを実践してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実践課題用のスペース\n",
    "# 自分のCSVファイルを読み込んで、学んだ技術を適用してみてください\n",
    "\n",
    "# 例:\n",
    "# my_data = pd.read_csv('my_data.csv')\n",
    "# quality_report = data_quality_report(my_data)\n",
    "# print(quality_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}