{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 6: データ型と欠損値処理 - ベーシック編\n",
    "\n",
    "## 今日の目標\n",
    "- 高度な欠損値処理手法を習得する\n",
    "- 欠損値パターンの分析方法を学ぶ\n",
    "- 実際のデータセットを使った実践的な処理を行う\n",
    "- パフォーマンスを考慮したデータ型最適化\n",
    "\n",
    "## アジェンダ\n",
    "1. 高度な欠損値補完手法\n",
    "2. 欠損値パターンの分析\n",
    "3. 時系列データの欠損値処理\n",
    "4. 機械学習による欠損値補完\n",
    "5. パフォーマンス最適化\n",
    "6. 実践的なケーススタディ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 必要なライブラリの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ライブラリの読み込み完了！\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 表示設定\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "print(\"ライブラリの読み込み完了！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 高度な欠損値補完手法\n",
    "\n",
    "### 2.1 K近傍法による補完（KNN Imputation）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不動産データ（最初の10行）:\n",
      "        面積  部屋数     築年数         価格  最寄り駅距離  階数\n",
      "0   94.901  4.0   5.358  4.758e+07   2.151  14\n",
      "1   75.852  3.0   3.194  3.797e+07   2.358  13\n",
      "2      NaN  5.0   0.580  4.990e+07   8.232   4\n",
      "3  125.691  6.0  20.004  6.270e+07   3.086   2\n",
      "4   72.975  2.0  16.761  3.629e+07   5.488  10\n",
      "5   72.976  2.0  81.724  3.491e+07  10.903  12\n",
      "6  127.376  5.0  56.949  6.283e+07   2.741   7\n",
      "7  103.023  4.0   8.107  5.176e+07   4.866   8\n",
      "8   65.916  3.0  14.653  3.290e+07  15.472   2\n",
      "9   96.277  6.0  28.962  4.783e+07   8.273   5\n",
      "\n",
      "欠損値の状況:\n",
      "面積        20\n",
      "部屋数        0\n",
      "築年数        0\n",
      "価格        30\n",
      "最寄り駅距離    16\n",
      "階数         0\n",
      "dtype: int64\n",
      "\n",
      "相関係数:\n",
      "           面積    部屋数    築年数     価格  最寄り駅距離     階数\n",
      "面積      1.000  0.884  0.087  1.000  -0.041  0.062\n",
      "部屋数     0.884  1.000  0.048  0.890  -0.070 -0.002\n",
      "築年数     0.087  0.048  1.000  0.040  -0.020  0.120\n",
      "価格      1.000  0.890  0.040  1.000  -0.016  0.056\n",
      "最寄り駅距離 -0.041 -0.070 -0.020 -0.016   1.000 -0.036\n",
      "階数      0.062 -0.002  0.120  0.056  -0.036  1.000\n"
     ]
    }
   ],
   "source": [
    "# サンプルデータの作成（不動産データ）\n",
    "np.random.seed(42)\n",
    "n_properties = 200\n",
    "\n",
    "# 相関のあるデータを生成\n",
    "area = np.random.normal(80, 30, n_properties)  # 面積 (m²)\n",
    "rooms = np.clip(np.round(area / 25 + np.random.normal(0, 0.5, n_properties)), 1, 6)  # 部屋数\n",
    "age = np.random.exponential(10, n_properties)  # 築年数\n",
    "price = (area * 50 + rooms * 5 - age * 2 + np.random.normal(0, 10, n_properties)) * 10000  # 価格\n",
    "\n",
    "real_estate_data = pd.DataFrame({\n",
    "    '面積': area,\n",
    "    '部屋数': rooms,\n",
    "    '築年数': age,\n",
    "    '価格': price,\n",
    "    '最寄り駅距離': np.random.gamma(2, 3, n_properties),  # 分\n",
    "    '階数': np.random.randint(1, 15, n_properties)\n",
    "})\n",
    "\n",
    "# 欠損値を意図的に作成（MCAR: Missing Completely At Random）\n",
    "missing_indices = np.random.choice(n_properties, size=int(n_properties * 0.15), replace=False)\n",
    "real_estate_data.loc[missing_indices, '価格'] = np.nan\n",
    "\n",
    "missing_indices = np.random.choice(n_properties, size=int(n_properties * 0.1), replace=False)\n",
    "real_estate_data.loc[missing_indices, '面積'] = np.nan\n",
    "\n",
    "missing_indices = np.random.choice(n_properties, size=int(n_properties * 0.08), replace=False)\n",
    "real_estate_data.loc[missing_indices, '最寄り駅距離'] = np.nan\n",
    "\n",
    "print(\"不動産データ（最初の10行）:\")\n",
    "print(real_estate_data.head(10))\n",
    "print(\"\\n欠損値の状況:\")\n",
    "print(real_estate_data.isnull().sum())\n",
    "print(\"\\n相関係数:\")\n",
    "print(real_estate_data.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN補完後の欠損値数:\n",
      "面積        0\n",
      "部屋数       0\n",
      "築年数       0\n",
      "価格        0\n",
      "最寄り駅距離    0\n",
      "階数        0\n",
      "dtype: int64\n",
      "\n",
      "価格の統計（補完前後比較）:\n",
      "元データ（欠損値除く）:\n",
      "count    1.700e+02\n",
      "mean     3.979e+07\n",
      "std      1.404e+07\n",
      "min      1.007e+07\n",
      "25%      2.922e+07\n",
      "50%      4.054e+07\n",
      "75%      4.751e+07\n",
      "max      8.001e+07\n",
      "Name: 価格, dtype: float64\n",
      "\n",
      "KNN補完後:\n",
      "count    2.000e+02\n",
      "mean     3.966e+07\n",
      "std      1.329e+07\n",
      "min      1.007e+07\n",
      "25%      3.112e+07\n",
      "50%      4.019e+07\n",
      "75%      4.705e+07\n",
      "max      8.001e+07\n",
      "Name: 価格, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# KNN補完の実行\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# KNN補完器の設定（k=5を使用）\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# データをコピーして補完\n",
    "real_estate_knn = real_estate_data.copy()\n",
    "real_estate_knn_filled = pd.DataFrame(\n",
    "    knn_imputer.fit_transform(real_estate_knn),\n",
    "    columns=real_estate_knn.columns\n",
    ")\n",
    "\n",
    "print(\"KNN補完後の欠損値数:\")\n",
    "print(real_estate_knn_filled.isnull().sum())\n",
    "\n",
    "# 補完前後の統計比較\n",
    "print(\"\\n価格の統計（補完前後比較）:\")\n",
    "print(\"元データ（欠損値除く）:\")\n",
    "print(real_estate_data['価格'].describe())\n",
    "print(\"\\nKNN補完後:\")\n",
    "print(real_estate_knn_filled['価格'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 回帰による補完（Iterative Imputation）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回帰による補完\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# 回帰補完器の設定\n",
    "iterative_imputer = IterativeImputer(\n",
    "    estimator=RandomForestRegressor(n_estimators=10, random_state=42),\n",
    "    random_state=42,\n",
    "    max_iter=10\n",
    ")\n",
    "\n",
    "# 回帰補完の実行\n",
    "real_estate_iterative = real_estate_data.copy()\n",
    "real_estate_iterative_filled = pd.DataFrame(\n",
    "    iterative_imputer.fit_transform(real_estate_iterative),\n",
    "    columns=real_estate_iterative.columns\n",
    ")\n",
    "\n",
    "print(\"回帰補完後の欠損値数:\")\n",
    "print(real_estate_iterative_filled.isnull().sum())\n",
    "\n",
    "print(\"\\n価格の統計（回帰補完）:\")\n",
    "print(real_estate_iterative_filled['価格'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 補完手法の比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 異なる補完手法の比較\n",
    "# 1. 平均値補完\n",
    "real_estate_mean = real_estate_data.copy()\n",
    "for col in real_estate_mean.select_dtypes(include=[np.number]).columns:\n",
    "    real_estate_mean[col] = real_estate_mean[col].fillna(real_estate_mean[col].mean())\n",
    "\n",
    "# 2. 中央値補完\n",
    "real_estate_median = real_estate_data.copy()\n",
    "for col in real_estate_median.select_dtypes(include=[np.number]).columns:\n",
    "    real_estate_median[col] = real_estate_median[col].fillna(real_estate_median[col].median())\n",
    "\n",
    "# 補完手法の比較可視化\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 価格の分布比較\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.hist(real_estate_data['価格'].dropna(), bins=20, alpha=0.7, label='元データ', density=True)\n",
    "plt.hist(real_estate_mean['価格'], bins=20, alpha=0.7, label='平均値補完', density=True)\n",
    "plt.title('価格分布: 平均値補完')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.hist(real_estate_data['価格'].dropna(), bins=20, alpha=0.7, label='元データ', density=True)\n",
    "plt.hist(real_estate_median['価格'], bins=20, alpha=0.7, label='中央値補完', density=True)\n",
    "plt.title('価格分布: 中央値補完')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.hist(real_estate_data['価格'].dropna(), bins=20, alpha=0.7, label='元データ', density=True)\n",
    "plt.hist(real_estate_knn_filled['価格'], bins=20, alpha=0.7, label='KNN補完', density=True)\n",
    "plt.title('価格分布: KNN補完')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.hist(real_estate_data['価格'].dropna(), bins=20, alpha=0.7, label='元データ', density=True)\n",
    "plt.hist(real_estate_iterative_filled['価格'], bins=20, alpha=0.7, label='回帰補完', density=True)\n",
    "plt.title('価格分布: 回帰補完')\n",
    "plt.legend()\n",
    "\n",
    "# 相関係数の比較\n",
    "plt.subplot(2, 3, 5)\n",
    "methods = ['元データ', '平均値', '中央値', 'KNN', '回帰']\n",
    "correlations = [\n",
    "    real_estate_data.corr().loc['価格', '面積'],\n",
    "    real_estate_mean.corr().loc['価格', '面積'],\n",
    "    real_estate_median.corr().loc['価格', '面積'],\n",
    "    real_estate_knn_filled.corr().loc['価格', '面積'],\n",
    "    real_estate_iterative_filled.corr().loc['価格', '面積']\n",
    "]\n",
    "\n",
    "plt.bar(methods, correlations)\n",
    "plt.title('価格と面積の相関係数比較')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('相関係数')\n",
    "\n",
    "# 統計値の比較表\n",
    "plt.subplot(2, 3, 6)\n",
    "stats_comparison = pd.DataFrame({\n",
    "    '平均値補完': real_estate_mean['価格'].describe(),\n",
    "    '中央値補完': real_estate_median['価格'].describe(),\n",
    "    'KNN補完': real_estate_knn_filled['価格'].describe(),\n",
    "    '回帰補完': real_estate_iterative_filled['価格'].describe()\n",
    "})\n",
    "\n",
    "# 統計値の比較をテキストで表示\n",
    "plt.text(0.1, 0.5, stats_comparison.round(2).to_string(), \n",
    "         transform=plt.gca().transAxes, fontsize=8, verticalalignment='center')\n",
    "plt.title('統計値比較')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n補完手法別の統計比較:\")\n",
    "print(stats_comparison.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 欠損値パターンの分析\n",
    "\n",
    "### 3.1 欠損値の種類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 異なる種類の欠損値パターンを作成\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# ベースデータの作成\n",
    "income = np.random.lognormal(mean=6, sigma=0.5, size=n_samples)  # 年収（万円）\n",
    "age = np.random.randint(22, 65, n_samples)  # 年齢\n",
    "education_years = np.random.randint(12, 20, n_samples)  # 教育年数\n",
    "satisfaction = np.random.randint(1, 11, n_samples)  # 満足度 (1-10)\n",
    "\n",
    "missing_patterns_data = pd.DataFrame({\n",
    "    '年収': income,\n",
    "    '年齢': age,\n",
    "    '教育年数': education_years,\n",
    "    '満足度': satisfaction\n",
    "})\n",
    "\n",
    "# MCAR (Missing Completely At Random): 完全にランダムな欠損\n",
    "mcar_indices = np.random.choice(n_samples, size=int(n_samples * 0.1), replace=False)\n",
    "missing_patterns_data.loc[mcar_indices, '満足度'] = np.nan\n",
    "\n",
    "# MAR (Missing At Random): 他の観測値に依存する欠損\n",
    "# 高年収者は年収を回答しない傾向があると仮定\n",
    "high_income_mask = missing_patterns_data['年収'] > missing_patterns_data['年収'].quantile(0.8)\n",
    "mar_indices = missing_patterns_data[high_income_mask].sample(frac=0.3).index\n",
    "missing_patterns_data.loc[mar_indices, '年収'] = np.nan\n",
    "\n",
    "# MNAR (Missing Not At Random): 欠損値自体が値に依存\n",
    "# 教育年数が低い人は教育年数を回答しない傾向があると仮定\n",
    "low_education_mask = missing_patterns_data['教育年数'] < 15\n",
    "mnar_indices = missing_patterns_data[low_education_mask].sample(frac=0.2).index\n",
    "missing_patterns_data.loc[mnar_indices, '教育年数'] = np.nan\n",
    "\n",
    "print(\"欠損値パターンの分析データ:\")\n",
    "print(missing_patterns_data.isnull().sum())\n",
    "print(\"\\n欠損値割合:\")\n",
    "print((missing_patterns_data.isnull().sum() / len(missing_patterns_data) * 100).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 欠損値パターンの可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 欠損値パターンの詳細分析\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 1. 欠損値マトリックス\n",
    "plt.subplot(2, 3, 1)\n",
    "sns.heatmap(missing_patterns_data.isnull(), cbar=True, yticklabels=False)\n",
    "plt.title('欠損値パターンマトリックス')\n",
    "\n",
    "# 2. 欠損値の共起パターン\n",
    "plt.subplot(2, 3, 2)\n",
    "missing_combinations = missing_patterns_data.isnull().groupby(\n",
    "    missing_patterns_data.isnull().apply(tuple, axis=1)\n",
    ").size().sort_values(ascending=False)\n",
    "\n",
    "top_patterns = missing_combinations.head(10)\n",
    "pattern_labels = [str(pattern) for pattern in top_patterns.index]\n",
    "plt.bar(range(len(top_patterns)), top_patterns.values)\n",
    "plt.title('欠損値パターンの頻度')\n",
    "plt.xlabel('パターン')\n",
    "plt.ylabel('頻度')\n",
    "plt.xticks(range(len(top_patterns)), \n",
    "           [f'パターン{i+1}' for i in range(len(top_patterns))], \n",
    "           rotation=45)\n",
    "\n",
    "# 3. 年収の欠損パターン分析（MAR分析）\n",
    "plt.subplot(2, 3, 3)\n",
    "income_complete = missing_patterns_data.dropna(subset=['年収'])['年収']\n",
    "all_ages = missing_patterns_data['年齢']\n",
    "ages_with_income = missing_patterns_data.dropna(subset=['年収'])['年齢']\n",
    "ages_without_income = missing_patterns_data[missing_patterns_data['年収'].isnull()]['年齢']\n",
    "\n",
    "plt.hist(ages_with_income, bins=20, alpha=0.7, label='年収回答あり', density=True)\n",
    "plt.hist(ages_without_income, bins=20, alpha=0.7, label='年収回答なし', density=True)\n",
    "plt.title('年収回答状況別の年齢分布')\n",
    "plt.xlabel('年齢')\n",
    "plt.ylabel('密度')\n",
    "plt.legend()\n",
    "\n",
    "# 4. 教育年数の欠損パターン分析（MNAR分析）\n",
    "plt.subplot(2, 3, 4)\n",
    "edu_complete = missing_patterns_data.dropna(subset=['教育年数'])['教育年数']\n",
    "edu_missing_ages = missing_patterns_data[missing_patterns_data['教育年数'].isnull()]['年齢']\n",
    "edu_complete_ages = missing_patterns_data.dropna(subset=['教育年数'])['年齢']\n",
    "\n",
    "plt.hist(edu_complete_ages, bins=15, alpha=0.7, label='教育年数回答あり', density=True)\n",
    "plt.hist(edu_missing_ages, bins=15, alpha=0.7, label='教育年数回答なし', density=True)\n",
    "plt.title('教育年数回答状況別の年齢分布')\n",
    "plt.xlabel('年齢')\n",
    "plt.ylabel('密度')\n",
    "plt.legend()\n",
    "\n",
    "# 5. 相関分析：欠損値と他の変数の関係\n",
    "plt.subplot(2, 3, 5)\n",
    "# 欠損値フラグを作成\n",
    "missing_flags = missing_patterns_data.isnull().astype(int)\n",
    "missing_flags.columns = [f'{col}_欠損' for col in missing_flags.columns]\n",
    "\n",
    "# 数値データと欠損フラグの相関\n",
    "combined_data = pd.concat([\n",
    "    missing_patterns_data.select_dtypes(include=[np.number]),\n",
    "    missing_flags\n",
    "], axis=1)\n",
    "\n",
    "correlation_with_missing = combined_data.corr()\n",
    "missing_corr = correlation_with_missing.loc[\n",
    "    combined_data.select_dtypes(include=[np.number]).columns[:4],\n",
    "    missing_flags.columns\n",
    "]\n",
    "\n",
    "sns.heatmap(missing_corr, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('変数と欠損パターンの相関')\n",
    "\n",
    "# 6. Little's MCAR test の結果（疑似）\n",
    "plt.subplot(2, 3, 6)\n",
    "mcar_results = pd.DataFrame({\n",
    "    '変数': ['満足度', '年収', '教育年数'],\n",
    "    'MCAR_p値': [0.456, 0.023, 0.012],  # 疑似的な値\n",
    "    '欠損タイプ': ['MCAR', 'MAR', 'MNAR']\n",
    "})\n",
    "\n",
    "colors = ['green' if p > 0.05 else 'red' for p in mcar_results['MCAR_p値']]\n",
    "plt.bar(mcar_results['変数'], mcar_results['MCAR_p値'], color=colors, alpha=0.7)\n",
    "plt.axhline(y=0.05, color='red', linestyle='--', label='p=0.05')\n",
    "plt.title('欠損値ランダム性テスト')\n",
    "plt.xlabel('変数')\n",
    "plt.ylabel('p値')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n欠損値パターンの分析結果:\")\n",
    "print(\"1. 満足度: MCAR（完全にランダム）- p値が0.05以上\")\n",
    "print(\"2. 年収: MAR（他の変数に依存）- 高年収者が回答を避ける傾向\")\n",
    "print(\"3. 教育年数: MNAR（値自体に依存）- 低教育年数者が回答を避ける傾向\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 時系列データの欠損値処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 時系列データの作成（センサーデータを模擬）\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range('2024-01-01', periods=365, freq='D')\n",
    "\n",
    "# トレンドと季節性を持つデータ\n",
    "trend = np.linspace(100, 120, 365)\n",
    "seasonal = 10 * np.sin(2 * np.pi * np.arange(365) / 365.25)\n",
    "noise = np.random.normal(0, 2, 365)\n",
    "temperature = trend + seasonal + noise\n",
    "\n",
    "# 湿度（温度と逆相関）\n",
    "humidity = 80 - 0.3 * (temperature - 100) + np.random.normal(0, 3, 365)\n",
    "\n",
    "# 気圧\n",
    "pressure = 1013 + np.random.normal(0, 10, 365)\n",
    "\n",
    "timeseries_data = pd.DataFrame({\n",
    "    '日付': dates,\n",
    "    '温度': temperature,\n",
    "    '湿度': humidity,\n",
    "    '気圧': pressure\n",
    "})\n",
    "\n",
    "timeseries_data.set_index('日付', inplace=True)\n",
    "\n",
    "# 時系列的な欠損パターンを作成\n",
    "# 1. ランダムな単発欠損\n",
    "random_missing = np.random.choice(365, size=20, replace=False)\n",
    "timeseries_data.loc[timeseries_data.index[random_missing], '温度'] = np.nan\n",
    "\n",
    "# 2. 連続する欠損（センサー故障を模擬）\n",
    "consecutive_start = 100\n",
    "timeseries_data.loc[timeseries_data.index[consecutive_start:consecutive_start+7], '湿度'] = np.nan\n",
    "\n",
    "# 3. 周期的な欠損（メンテナンス期間を模擬）\n",
    "maintenance_days = range(0, 365, 30)  # 30日ごと\n",
    "timeseries_data.loc[timeseries_data.index[maintenance_days], '気圧'] = np.nan\n",
    "\n",
    "print(\"時系列データ（最初の10日）:\")\n",
    "print(timeseries_data.head(10))\n",
    "print(\"\\n欠損値の状況:\")\n",
    "print(timeseries_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 時系列データの欠損値補完手法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 時系列データの様々な補完手法\n",
    "ts_methods = {}\n",
    "\n",
    "# 1. 前方補完（Forward Fill）\n",
    "ts_methods['前方補完'] = timeseries_data.fillna(method='ffill')\n",
    "\n",
    "# 2. 後方補完（Backward Fill）\n",
    "ts_methods['後方補完'] = timeseries_data.fillna(method='bfill')\n",
    "\n",
    "# 3. 線形補間\n",
    "ts_methods['線形補間'] = timeseries_data.interpolate(method='linear')\n",
    "\n",
    "# 4. 時系列補間\n",
    "ts_methods['時間補間'] = timeseries_data.interpolate(method='time')\n",
    "\n",
    "# 5. スプライン補間\n",
    "ts_methods['スプライン補間'] = timeseries_data.interpolate(method='spline', order=3)\n",
    "\n",
    "# 6. 移動平均補完\n",
    "ts_rolling = timeseries_data.copy()\n",
    "for col in ts_rolling.columns:\n",
    "    rolling_mean = ts_rolling[col].rolling(window=7, center=True).mean()\n",
    "    ts_rolling[col] = ts_rolling[col].fillna(rolling_mean)\n",
    "ts_methods['移動平均補完'] = ts_rolling.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# 結果の可視化\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown']\n",
    "method_names = list(ts_methods.keys())\n",
    "\n",
    "for i, (method_name, data) in enumerate(ts_methods.items()):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # 温度データをプロット（1月のみ）\n",
    "    jan_data = data['2024-01-01':'2024-01-31']\n",
    "    original_jan = timeseries_data['2024-01-01':'2024-01-31']\n",
    "    \n",
    "    ax.plot(jan_data.index, jan_data['温度'], color=colors[i], label=method_name, linewidth=2)\n",
    "    \n",
    "    # 欠損値の位置をマーク\n",
    "    missing_mask = original_jan['温度'].isnull()\n",
    "    if missing_mask.any():\n",
    "        ax.scatter(original_jan[missing_mask].index, \n",
    "                  jan_data[missing_mask]['温度'], \n",
    "                  color='red', s=50, zorder=5, label='補完値')\n",
    "    \n",
    "    ax.set_title(f'{method_name}による補完')\n",
    "    ax.set_ylabel('温度')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 補完手法の評価\n",
    "print(\"\\n補完手法の統計比較（温度）:\")\n",
    "temp_stats = pd.DataFrame({\n",
    "    method: data['温度'].describe() \n",
    "    for method, data in ts_methods.items()\n",
    "})\n",
    "print(temp_stats.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 季節性を考慮した補完"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 季節性を考慮した高度な補完\n",
    "from scipy import stats\n",
    "\n",
    "def seasonal_imputation(series, period=7):\n",
    "    \"\"\"季節性を考慮した欠損値補完\"\"\"\n",
    "    result = series.copy()\n",
    "    \n",
    "    for i in range(len(series)):\n",
    "        if pd.isna(series.iloc[i]):\n",
    "            # 同じ曜日（または周期）のデータを取得\n",
    "            same_period_indices = range(i % period, len(series), period)\n",
    "            same_period_values = series.iloc[same_period_indices].dropna()\n",
    "            \n",
    "            if len(same_period_values) > 0:\n",
    "                # 近傍の値の重み付き平均を使用\n",
    "                weights = np.exp(-np.abs(np.array(same_period_indices) - i) / 30)\n",
    "                weights = weights[:len(same_period_values)]\n",
    "                \n",
    "                result.iloc[i] = np.average(same_period_values, weights=weights)\n",
    "            else:\n",
    "                # フォールバック: 線形補間\n",
    "                result.iloc[i] = series.interpolate().iloc[i]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# 季節性補完の適用\n",
    "ts_seasonal = timeseries_data.copy()\n",
    "for col in ts_seasonal.columns:\n",
    "    ts_seasonal[col] = seasonal_imputation(ts_seasonal[col], period=7)\n",
    "\n",
    "# 結果の比較\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# 温度データの比較（2月のデータ）\n",
    "feb_original = timeseries_data['2024-02-01':'2024-02-29']\n",
    "feb_linear = ts_methods['線形補間']['2024-02-01':'2024-02-29']\n",
    "feb_seasonal = ts_seasonal['2024-02-01':'2024-02-29']\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(feb_linear.index, feb_linear['温度'], 'b-', label='線形補間', linewidth=2)\n",
    "plt.plot(feb_seasonal.index, feb_seasonal['温度'], 'r-', label='季節性補間', linewidth=2)\n",
    "\n",
    "# 欠損値の位置をマーク\n",
    "missing_mask = feb_original['温度'].isnull()\n",
    "if missing_mask.any():\n",
    "    plt.scatter(feb_original[missing_mask].index, \n",
    "               feb_linear[missing_mask]['温度'], \n",
    "               color='blue', s=50, zorder=5, label='線形補完値')\n",
    "    plt.scatter(feb_original[missing_mask].index, \n",
    "               feb_seasonal[missing_mask]['温度'], \n",
    "               color='red', s=50, zorder=5, label='季節性補完値')\n",
    "\n",
    "plt.title('補完手法の比較（2月の温度データ）')\n",
    "plt.ylabel('温度')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 湿度データの連続欠損補完比較\n",
    "plt.subplot(2, 1, 2)\n",
    "apr_period = slice('2024-04-05', '2024-04-20')\n",
    "plt.plot(timeseries_data.loc[apr_period].index, \n",
    "         ts_methods['線形補間'].loc[apr_period]['湿度'], 'b-', \n",
    "         label='線形補間', linewidth=2)\n",
    "plt.plot(timeseries_data.loc[apr_period].index, \n",
    "         ts_seasonal.loc[apr_period]['湿度'], 'r-', \n",
    "         label='季節性補間', linewidth=2)\n",
    "\n",
    "# 連続欠損期間をハイライト\n",
    "consecutive_period = slice('2024-04-10', '2024-04-17')\n",
    "plt.axvspan(timeseries_data.loc[consecutive_period].index[0], \n",
    "           timeseries_data.loc[consecutive_period].index[-1], \n",
    "           alpha=0.3, color='yellow', label='連続欠損期間')\n",
    "\n",
    "plt.title('連続欠損の補完比較（湿度データ）')\n",
    "plt.ylabel('湿度')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 機械学習による欠損値補完の評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 補完精度の評価実験\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "def evaluate_imputation_methods(data, target_column, missing_ratio=0.2, n_splits=5):\n",
    "    \"\"\"欠損値補完手法の評価\"\"\"\n",
    "    \n",
    "    # 完全なデータを準備\n",
    "    complete_data = data.dropna()\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    results = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(complete_data)):\n",
    "        # データを分割\n",
    "        train_data = complete_data.iloc[train_idx].copy()\n",
    "        test_data = complete_data.iloc[test_idx].copy()\n",
    "        \n",
    "        # テストデータに人工的な欠損を作成\n",
    "        n_missing = int(len(test_data) * missing_ratio)\n",
    "        missing_idx = np.random.choice(len(test_data), n_missing, replace=False)\n",
    "        \n",
    "        # 真の値を保存\n",
    "        true_values = test_data.iloc[missing_idx][target_column].values\n",
    "        \n",
    "        # 欠損を作成\n",
    "        test_data_missing = test_data.copy()\n",
    "        test_data_missing.iloc[missing_idx, test_data_missing.columns.get_loc(target_column)] = np.nan\n",
    "        \n",
    "        # 様々な補完手法を適用\n",
    "        methods_performance = {}\n",
    "        \n",
    "        # 1. 平均値補完\n",
    "        mean_filled = test_data_missing.copy()\n",
    "        mean_filled[target_column] = mean_filled[target_column].fillna(train_data[target_column].mean())\n",
    "        pred_mean = mean_filled.iloc[missing_idx][target_column].values\n",
    "        methods_performance['平均値'] = {\n",
    "            'MAE': mean_absolute_error(true_values, pred_mean),\n",
    "            'R2': r2_score(true_values, pred_mean)\n",
    "        }\n",
    "        \n",
    "        # 2. KNN補完\n",
    "        knn_imputer = KNNImputer(n_neighbors=5)\n",
    "        knn_filled = pd.DataFrame(\n",
    "            knn_imputer.fit_transform(test_data_missing),\n",
    "            columns=test_data_missing.columns,\n",
    "            index=test_data_missing.index\n",
    "        )\n",
    "        pred_knn = knn_filled.iloc[missing_idx][target_column].values\n",
    "        methods_performance['KNN'] = {\n",
    "            'MAE': mean_absolute_error(true_values, pred_knn),\n",
    "            'R2': r2_score(true_values, pred_knn)\n",
    "        }\n",
    "        \n",
    "        # 3. 回帰補完\n",
    "        iterative_imputer = IterativeImputer(\n",
    "            estimator=RandomForestRegressor(n_estimators=10, random_state=42),\n",
    "            random_state=42\n",
    "        )\n",
    "        iterative_filled = pd.DataFrame(\n",
    "            iterative_imputer.fit_transform(test_data_missing),\n",
    "            columns=test_data_missing.columns,\n",
    "            index=test_data_missing.index\n",
    "        )\n",
    "        pred_iterative = iterative_filled.iloc[missing_idx][target_column].values\n",
    "        methods_performance['回帰'] = {\n",
    "            'MAE': mean_absolute_error(true_values, pred_iterative),\n",
    "            'R2': r2_score(true_values, pred_iterative)\n",
    "        }\n",
    "        \n",
    "        results.append(methods_performance)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 不動産データで評価実験を実行\n",
    "evaluation_results = evaluate_imputation_methods(\n",
    "    real_estate_data, '価格', missing_ratio=0.2, n_splits=5\n",
    ")\n",
    "\n",
    "# 結果の集計\n",
    "methods = ['平均値', 'KNN', '回帰']\n",
    "metrics = ['MAE', 'R2']\n",
    "\n",
    "summary_results = {}\n",
    "for method in methods:\n",
    "    summary_results[method] = {}\n",
    "    for metric in metrics:\n",
    "        values = [result[method][metric] for result in evaluation_results]\n",
    "        summary_results[method][metric] = {\n",
    "            'mean': np.mean(values),\n",
    "            'std': np.std(values)\n",
    "        }\n",
    "\n",
    "# 結果の可視化\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "mae_means = [summary_results[method]['MAE']['mean'] for method in methods]\n",
    "mae_stds = [summary_results[method]['MAE']['std'] for method in methods]\n",
    "plt.bar(methods, mae_means, yerr=mae_stds, capsize=5)\n",
    "plt.title('平均絶対誤差（MAE）の比較')\n",
    "plt.ylabel('MAE')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "r2_means = [summary_results[method]['R2']['mean'] for method in methods]\n",
    "r2_stds = [summary_results[method]['R2']['std'] for method in methods]\n",
    "plt.bar(methods, r2_means, yerr=r2_stds, capsize=5)\n",
    "plt.title('決定係数（R²）の比較')\n",
    "plt.ylabel('R²')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 数値結果の表示\n",
    "print(\"補完手法の評価結果（5-fold交差検証）:\")\n",
    "print(\"\\nMAE（平均絶対誤差）- 小さいほど良い:\")\n",
    "for method in methods:\n",
    "    mean_mae = summary_results[method]['MAE']['mean']\n",
    "    std_mae = summary_results[method]['MAE']['std']\n",
    "    print(f\"{method}: {mean_mae:.2f} ± {std_mae:.2f}\")\n",
    "\n",
    "print(\"\\nR²（決定係数）- 大きいほど良い:\")\n",
    "for method in methods:\n",
    "    mean_r2 = summary_results[method]['R2']['mean']\n",
    "    std_r2 = summary_results[method]['R2']['std']\n",
    "    print(f\"{method}: {mean_r2:.3f} ± {std_r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. パフォーマンス最適化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大規模データでのパフォーマンス比較\n",
    "import time\n",
    "from memory_profiler import profile\n",
    "\n",
    "def create_large_dataset(n_rows=10000, n_cols=20, missing_ratio=0.1):\n",
    "    \"\"\"大規模データセットの作成\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # データ生成\n",
    "    data = np.random.randn(n_rows, n_cols)\n",
    "    \n",
    "    # 相関のある列を作成\n",
    "    for i in range(1, n_cols):\n",
    "        data[:, i] = 0.5 * data[:, i-1] + 0.5 * data[:, i]\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=[f'feature_{i}' for i in range(n_cols)])\n",
    "    \n",
    "    # ランダムに欠損値を作成\n",
    "    for col in df.columns:\n",
    "        missing_mask = np.random.random(n_rows) < missing_ratio\n",
    "        df.loc[missing_mask, col] = np.nan\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 大規模データセットの作成\n",
    "large_data = create_large_dataset(n_rows=10000, n_cols=20, missing_ratio=0.1)\n",
    "print(f\"大規模データセット: {large_data.shape}\")\n",
    "print(f\"欠損値数: {large_data.isnull().sum().sum()}\")\n",
    "print(f\"メモリ使用量: {large_data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# パフォーマンス測定\n",
    "performance_results = {}\n",
    "\n",
    "# 1. 平均値補完\n",
    "start_time = time.time()\n",
    "mean_filled = large_data.fillna(large_data.mean())\n",
    "mean_time = time.time() - start_time\n",
    "performance_results['平均値補完'] = mean_time\n",
    "\n",
    "# 2. KNN補完（小さなk値）\n",
    "start_time = time.time()\n",
    "knn_imputer_fast = KNNImputer(n_neighbors=3)\n",
    "knn_filled_fast = pd.DataFrame(\n",
    "    knn_imputer_fast.fit_transform(large_data),\n",
    "    columns=large_data.columns\n",
    ")\n",
    "knn_fast_time = time.time() - start_time\n",
    "performance_results['KNN補完(k=3)'] = knn_fast_time\n",
    "\n",
    "# 3. 前方補完\n",
    "start_time = time.time()\n",
    "ffill_filled = large_data.fillna(method='ffill')\n",
    "ffill_time = time.time() - start_time\n",
    "performance_results['前方補完'] = ffill_time\n",
    "\n",
    "# 4. 線形補間\n",
    "start_time = time.time()\n",
    "interpolate_filled = large_data.interpolate()\n",
    "interpolate_time = time.time() - start_time\n",
    "performance_results['線形補間'] = interpolate_time\n",
    "\n",
    "# パフォーマンス結果の可視化\n",
    "plt.figure(figsize=(10, 6))\n",
    "methods = list(performance_results.keys())\n",
    "times = list(performance_results.values())\n",
    "\n",
    "bars = plt.bar(methods, times)\n",
    "plt.title('欠損値補完手法のパフォーマンス比較')\n",
    "plt.ylabel('実行時間（秒）')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# 各バーに実行時間を表示\n",
    "for bar, time_val in zip(bars, times):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{time_val:.2f}s', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nパフォーマンス結果:\")\n",
    "for method, time_val in performance_results.items():\n",
    "    print(f\"{method}: {time_val:.2f}秒\")\n",
    "\n",
    "# メモリ効率的なデータ型の推奨\n",
    "print(\"\\nメモリ最適化のための推奨事項:\")\n",
    "print(\"1. カテゴリカルデータ → category型\")\n",
    "print(\"2. 整数データ → 適切なint型（int8, int16, int32）\")\n",
    "print(\"3. 浮動小数点 → float32（精度が十分な場合）\")\n",
    "print(\"4. 大規模データ → チャンク処理またはDaskの利用\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 実践的なケーススタディ: 顧客購買データの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実際のビジネスシナリオを模擬したデータセット\n",
    "np.random.seed(42)\n",
    "n_customers = 5000\n",
    "\n",
    "# 顧客の基本情報\n",
    "ages = np.random.randint(18, 80, n_customers)\n",
    "genders = np.random.choice(['M', 'F', 'Other'], n_customers, p=[0.48, 0.48, 0.04])\n",
    "income_levels = np.random.choice(['Low', 'Medium', 'High'], n_customers, p=[0.3, 0.5, 0.2])\n",
    "\n",
    "# 収入（カテゴリに基づく）\n",
    "income_mapping = {'Low': (200, 400), 'Medium': (400, 800), 'High': (800, 1500)}\n",
    "incomes = []\n",
    "for level in income_levels:\n",
    "    low, high = income_mapping[level]\n",
    "    incomes.append(np.random.uniform(low, high))\n",
    "\n",
    "# 購買行動（年齢と収入に依存）\n",
    "purchase_frequency = (ages / 100 + np.array(incomes) / 1000 + np.random.normal(0, 0.2, n_customers)) * 50\n",
    "purchase_frequency = np.clip(purchase_frequency, 0, 200)\n",
    "\n",
    "# 満足度（購買頻度と相関）\n",
    "satisfaction = 1 + 4 * (purchase_frequency / 200) + np.random.normal(0, 0.5, n_customers)\n",
    "satisfaction = np.clip(satisfaction, 1, 5)\n",
    "\n",
    "# 地域\n",
    "regions = np.random.choice(['北海道', '関東', '関西', '九州', '其他'], n_customers, \n",
    "                          p=[0.1, 0.4, 0.25, 0.15, 0.1])\n",
    "\n",
    "# データフレームの作成\n",
    "customer_business_data = pd.DataFrame({\n",
    "    '顧客ID': range(1, n_customers + 1),\n",
    "    '年齢': ages,\n",
    "    '性別': genders,\n",
    "    '年収': incomes,\n",
    "    '収入レベル': income_levels,\n",
    "    '購買頻度': purchase_frequency,\n",
    "    '満足度': satisfaction,\n",
    "    '地域': regions\n",
    "})\n",
    "\n",
    "# 現実的な欠損パターンを作成\n",
    "\n",
    "# 1. 年収の欠損（プライバシーを理由とした非回答）\n",
    "# 高年収者と女性の方が回答率が低い傾向\n",
    "income_missing_prob = 0.15 + 0.1 * (customer_business_data['年収'] > 800) + 0.05 * (customer_business_data['性別'] == 'F')\n",
    "income_missing_mask = np.random.random(n_customers) < income_missing_prob\n",
    "customer_business_data.loc[income_missing_mask, '年収'] = np.nan\n",
    "\n",
    "# 2. 満足度の欠損（不満足な顧客が回答しない傾向）\n",
    "satisfaction_missing_prob = 0.1 + 0.2 * (customer_business_data['満足度'] < 3)\n",
    "satisfaction_missing_mask = np.random.random(n_customers) < satisfaction_missing_prob\n",
    "customer_business_data.loc[satisfaction_missing_mask, '満足度'] = np.nan\n",
    "\n",
    "# 3. 購買頻度の欠損（ランダム）\n",
    "freq_missing_mask = np.random.random(n_customers) < 0.05\n",
    "customer_business_data.loc[freq_missing_mask, '購買頻度'] = np.nan\n",
    "\n",
    "print(\"ビジネスケース：顧客購買データ\")\n",
    "print(f\"データサイズ: {customer_business_data.shape}\")\n",
    "print(\"\\n欠損値の状況:\")\n",
    "print(customer_business_data.isnull().sum())\n",
    "print(\"\\n欠損率（%）:\")\n",
    "print((customer_business_data.isnull().sum() / len(customer_business_data) * 100).round(2))\n",
    "\n",
    "print(\"\\nデータの概要:\")\n",
    "print(customer_business_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 ビジネスロジックに基づく包括的な前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 包括的なデータ前処理パイプライン\n",
    "def comprehensive_preprocessing(df):\n",
    "    \"\"\"ビジネス要件を考慮した包括的な前処理\"\"\"\n",
    "    \n",
    "    processed_df = df.copy()\n",
    "    preprocessing_log = []\n",
    "    \n",
    "    # 1. データ型の最適化\n",
    "    # 年齢: int8で十分\n",
    "    processed_df['年齢'] = processed_df['年齢'].astype('int8')\n",
    "    \n",
    "    # 性別と地域: category型\n",
    "    processed_df['性別'] = processed_df['性別'].astype('category')\n",
    "    processed_df['地域'] = processed_df['地域'].astype('category')\n",
    "    processed_df['収入レベル'] = processed_df['収入レベル'].astype('category')\n",
    "    \n",
    "    # 年収: float32で十分\n",
    "    processed_df['年収'] = processed_df['年収'].astype('float32')\n",
    "    \n",
    "    preprocessing_log.append(\"データ型を最適化\")\n",
    "    \n",
    "    # 2. 年収の欠損値処理\n",
    "    # 年齢・性別・地域グループごとの中央値で補完\n",
    "    def fill_income(row):\n",
    "        if pd.isna(row['年収']):\n",
    "            # 同じ属性グループの中央値を使用\n",
    "            mask = (\n",
    "                (processed_df['年齢'] >= row['年齢'] - 5) & \n",
    "                (processed_df['年齢'] <= row['年齢'] + 5) &\n",
    "                (processed_df['性別'] == row['性別']) &\n",
    "                (processed_df['地域'] == row['地域'])\n",
    "            )\n",
    "            \n",
    "            group_incomes = processed_df.loc[mask, '年収'].dropna()\n",
    "            \n",
    "            if len(group_incomes) >= 5:  # 十分なサンプルがある場合\n",
    "                return group_incomes.median()\n",
    "            else:  # サンプルが少ない場合は収入レベルの中央値を使用\n",
    "                level_incomes = processed_df[\n",
    "                    processed_df['収入レベル'] == row['収入レベル']\n",
    "                ]['年収'].dropna()\n",
    "                return level_incomes.median() if len(level_incomes) > 0 else processed_df['年収'].median()\n",
    "        return row['年収']\n",
    "    \n",
    "    processed_df['年収'] = processed_df.apply(fill_income, axis=1)\n",
    "    preprocessing_log.append(\"年収の欠損値を同属性グループの中央値で補完\")\n",
    "    \n",
    "    # 3. 購買頻度の欠損値処理\n",
    "    # 年収と年齢の関係から予測\n",
    "    freq_complete_mask = processed_df['購買頻度'].notna()\n",
    "    if freq_complete_mask.sum() > 100:  # 十分なデータがある場合\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        \n",
    "        # 特徴量の準備\n",
    "        X_complete = processed_df.loc[freq_complete_mask, ['年収', '年齢']]\n",
    "        y_complete = processed_df.loc[freq_complete_mask, '購買頻度']\n",
    "        \n",
    "        # 回帰モデルの訓練\n",
    "        freq_model = LinearRegression()\n",
    "        freq_model.fit(X_complete, y_complete)\n",
    "        \n",
    "        # 欠損値の予測\n",
    "        freq_missing_mask = processed_df['購買頻度'].isna()\n",
    "        X_missing = processed_df.loc[freq_missing_mask, ['年収', '年齢']]\n",
    "        \n",
    "        predicted_freq = freq_model.predict(X_missing)\n",
    "        processed_df.loc[freq_missing_mask, '購買頻度'] = predicted_freq\n",
    "        \n",
    "        preprocessing_log.append(\"購買頻度の欠損値を線形回帰で予測\")\n",
    "    else:\n",
    "        # データが少ない場合は中央値で補完\n",
    "        processed_df['購買頻度'] = processed_df['購買頻度'].fillna(\n",
    "            processed_df['購買頻度'].median()\n",
    "        )\n",
    "        preprocessing_log.append(\"購買頻度の欠損値を中央値で補完\")\n",
    "    \n",
    "    # 4. 満足度の欠損値処理\n",
    "    # 購買頻度との関係を考慮\n",
    "    satisfaction_median_by_freq = processed_df.groupby(\n",
    "        pd.cut(processed_df['購買頻度'], bins=5)\n",
    "    )['満足度'].median()\n",
    "    \n",
    "    def fill_satisfaction(row):\n",
    "        if pd.isna(row['満足度']):\n",
    "            freq_bin = pd.cut([row['購買頻度']], bins=5)[0]\n",
    "            if freq_bin in satisfaction_median_by_freq.index:\n",
    "                return satisfaction_median_by_freq[freq_bin]\n",
    "            else:\n",
    "                return processed_df['満足度'].median()\n",
    "        return row['満足度']\n",
    "    \n",
    "    processed_df['満足度'] = processed_df.apply(fill_satisfaction, axis=1)\n",
    "    preprocessing_log.append(\"満足度の欠損値を購買頻度レベル別中央値で補完\")\n",
    "    \n",
    "    # 5. 外れ値の処理\n",
    "    # 年収の外れ値をクリッピング\n",
    "    income_q99 = processed_df['年収'].quantile(0.99)\n",
    "    processed_df['年収'] = processed_df['年収'].clip(upper=income_q99)\n",
    "    preprocessing_log.append(f\"年収の外れ値を{income_q99:.0f}万円でクリッピング\")\n",
    "    \n",
    "    # 6. 新しい特徴量の作成\n",
    "    processed_df['年収_年齢比'] = processed_df['年収'] / processed_df['年齢']\n",
    "    processed_df['購買_満足度'] = processed_df['購買頻度'] * processed_df['満足度']\n",
    "    preprocessing_log.append(\"新しい特徴量を作成\")\n",
    "    \n",
    "    return processed_df, preprocessing_log\n",
    "\n",
    "# 前処理の実行\n",
    "processed_data, log = comprehensive_preprocessing(customer_business_data)\n",
    "\n",
    "print(\"前処理の実行ログ:\")\n",
    "for i, step in enumerate(log, 1):\n",
    "    print(f\"{i}. {step}\")\n",
    "\n",
    "print(\"\\n前処理後の欠損値:\")\n",
    "print(processed_data.isnull().sum())\n",
    "\n",
    "print(\"\\n前処理後のデータ型:\")\n",
    "print(processed_data.dtypes)\n",
    "\n",
    "print(\"\\nメモリ使用量の比較:\")\n",
    "original_memory = customer_business_data.memory_usage(deep=True).sum() / 1024**2\n",
    "processed_memory = processed_data.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"元データ: {original_memory:.2f} MB\")\n",
    "print(f\"処理後: {processed_memory:.2f} MB\")\n",
    "print(f\"削減率: {(1 - processed_memory/original_memory)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 前処理結果の検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理の効果を検証\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. 年収分布の比較\n",
    "axes[0, 0].hist(customer_business_data['年収'].dropna(), bins=30, alpha=0.7, \n",
    "               label='処理前', density=True)\n",
    "axes[0, 0].hist(processed_data['年収'], bins=30, alpha=0.7, \n",
    "               label='処理後', density=True)\n",
    "axes[0, 0].set_title('年収分布の比較')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. 満足度分布の比較\n",
    "axes[0, 1].hist(customer_business_data['満足度'].dropna(), bins=20, alpha=0.7, \n",
    "               label='処理前', density=True)\n",
    "axes[0, 1].hist(processed_data['満足度'], bins=20, alpha=0.7, \n",
    "               label='処理後', density=True)\n",
    "axes[0, 1].set_title('満足度分布の比較')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. 購買頻度分布の比較\n",
    "axes[0, 2].hist(customer_business_data['購買頻度'].dropna(), bins=30, alpha=0.7, \n",
    "               label='処理前', density=True)\n",
    "axes[0, 2].hist(processed_data['購買頻度'], bins=30, alpha=0.7, \n",
    "               label='処理後', density=True)\n",
    "axes[0, 2].set_title('購買頻度分布の比較')\n",
    "axes[0, 2].legend()\n",
    "\n",
    "# 4. 相関関係の比較\n",
    "original_corr = customer_business_data.select_dtypes(include=[np.number]).corr()\n",
    "processed_corr = processed_data.select_dtypes(include=[np.number]).corr()\n",
    "\n",
    "im1 = axes[1, 0].imshow(original_corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "axes[1, 0].set_title('処理前の相関マトリックス')\n",
    "axes[1, 0].set_xticks(range(len(original_corr.columns)))\n",
    "axes[1, 0].set_yticks(range(len(original_corr.columns)))\n",
    "axes[1, 0].set_xticklabels(original_corr.columns, rotation=45)\n",
    "axes[1, 0].set_yticklabels(original_corr.columns)\n",
    "\n",
    "im2 = axes[1, 1].imshow(processed_corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "axes[1, 1].set_title('処理後の相関マトリックス')\n",
    "axes[1, 1].set_xticks(range(len(processed_corr.columns)))\n",
    "axes[1, 1].set_yticks(range(len(processed_corr.columns)))\n",
    "axes[1, 1].set_xticklabels(processed_corr.columns, rotation=45)\n",
    "axes[1, 1].set_yticklabels(processed_corr.columns)\n",
    "\n",
    "# 5. 欠損値パターンの変化\n",
    "missing_before = customer_business_data.isnull().sum()\n",
    "missing_after = processed_data.isnull().sum()\n",
    "\n",
    "x = range(len(missing_before))\n",
    "width = 0.35\n",
    "\n",
    "axes[1, 2].bar([i - width/2 for i in x], missing_before, width, \n",
    "              label='処理前', alpha=0.7)\n",
    "axes[1, 2].bar([i + width/2 for i in x], missing_after, width, \n",
    "              label='処理後', alpha=0.7)\n",
    "axes[1, 2].set_title('欠損値数の変化')\n",
    "axes[1, 2].set_xlabel('列')\n",
    "axes[1, 2].set_ylabel('欠損値数')\n",
    "axes[1, 2].set_xticks(x)\n",
    "axes[1, 2].set_xticklabels(missing_before.index, rotation=45)\n",
    "axes[1, 2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 統計的検証\n",
    "print(\"\\n前処理の統計的検証:\")\n",
    "print(\"\\n1. データ完全性:\")\n",
    "completeness_before = (1 - customer_business_data.isnull().sum().sum() / \n",
    "                      (len(customer_business_data) * len(customer_business_data.columns))) * 100\n",
    "completeness_after = (1 - processed_data.isnull().sum().sum() / \n",
    "                     (len(processed_data) * len(processed_data.columns))) * 100\n",
    "print(f\"処理前: {completeness_before:.1f}%\")\n",
    "print(f\"処理後: {completeness_after:.1f}%\")\n",
    "\n",
    "print(\"\\n2. 主要な相関関係の保持:\")\n",
    "original_income_age_corr = customer_business_data[['年収', '年齢']].corr().iloc[0, 1]\n",
    "processed_income_age_corr = processed_data[['年収', '年齢']].corr().iloc[0, 1]\n",
    "print(f\"年収-年齢相関 処理前: {original_income_age_corr:.3f}\")\n",
    "print(f\"年収-年齢相関 処理後: {processed_income_age_corr:.3f}\")\n",
    "\n",
    "print(\"\\n3. 新しい特徴量の有効性:\")\n",
    "new_feature_corr = processed_data[['購買_満足度', '購買頻度', '満足度']].corr()\n",
    "print(f\"購買_満足度と購買頻度の相関: {new_feature_corr.loc['購買_満足度', '購買頻度']:.3f}\")\n",
    "print(f\"購買_満足度と満足度の相関: {new_feature_corr.loc['購買_満足度', '満足度']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. まとめと実践的なガイドライン\n",
    "\n",
    "### 今日学習した高度な技術\n",
    "1. **高度な補完手法**\n",
    "   - K近傍法（KNN）補完\n",
    "   - 回帰による反復補完\n",
    "   - 時系列データの季節性を考慮した補完\n",
    "\n",
    "2. **欠損値パターンの分析**\n",
    "   - MCAR/MAR/MNARの識別\n",
    "   - 欠損パターンの可視化\n",
    "   - 統計的検定による欠損機構の評価\n",
    "\n",
    "3. **パフォーマンス最適化**\n",
    "   - メモリ効率的なデータ型選択\n",
    "   - 処理速度の比較評価\n",
    "   - 大規模データでの実用的な手法選択\n",
    "\n",
    "### 実践的なガイドライン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実践的な欠損値処理のガイドライン\n",
    "print(\"\\n📋 実践的な欠損値処理ガイドライン\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "guidelines = {\n",
    "    \"1. 事前分析\": [\n",
    "        \"• 欠損値の割合と分布を確認\",\n",
    "        \"• 欠損パターンの可視化\",\n",
    "        \"• ビジネス要件との照合\"\n",
    "    ],\n",
    "    \"2. 欠損機構の判定\": [\n",
    "        \"• MCAR: ランダム削除 or 単純補完\",\n",
    "        \"• MAR: 高度な補完手法\",\n",
    "        \"• MNAR: ドメイン知識活用\"\n",
    "    ],\n",
    "    \"3. 手法選択の基準\": [\n",
    "        \"• データサイズ: 小→単純、大→高度\",\n",
    "        \"• 欠損率: <5%→削除、5-20%→補完、>20%→慎重検討\",\n",
    "        \"• 計算資源: 制限有→高速手法、無制限→高精度手法\"\n",
    "    ],\n",
    "    \"4. 手法別推奨シナリオ\": [\n",
    "        \"• 平均値/中央値: 単純な数値データ\",\n",
    "        \"• KNN: 相関の強いデータ\",\n",
    "        \"• 回帰: 高精度が必要な場合\",\n",
    "        \"• 時系列補間: 連続的な時系列データ\"\n",
    "    ],\n",
    "    \"5. 検証ポイント\": [\n",
    "        \"• 補完前後の分布比較\",\n",
    "        \"• 相関関係の保持確認\",\n",
    "        \"• 外れ値の発生確認\",\n",
    "        \"• ビジネス指標への影響評価\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, items in guidelines.items():\n",
    "    print(f\"\\n{category}\")\n",
    "    for item in items:\n",
    "        print(f\"  {item}\")\n",
    "\n",
    "print(\"\\n\\n🎯 最適な手法選択のフローチャート\")\n",
    "print(\"=\"*50)\n",
    "flowchart = \"\"\"\n",
    "データの欠損値を発見\n",
    "    ↓\n",
    "欠損率 < 5% ?\n",
    "    ↓ Yes → 行削除を検討\n",
    "    ↓ No\n",
    "計算資源に制約あり?\n",
    "    ↓ Yes → 単純補完（平均値/中央値/最頻値）\n",
    "    ↓ No\n",
    "データに相関関係あり?\n",
    "    ↓ Yes → KNN補完 or 回帰補完\n",
    "    ↓ No\n",
    "時系列データ?\n",
    "    ↓ Yes → 線形補間 or 季節性考慮補完\n",
    "    ↓ No\n",
    "カテゴリカルデータ?\n",
    "    ↓ Yes → 最頻値 or 新カテゴリ作成\n",
    "    ↓ No\n",
    "ビジネスルールの適用\n",
    "\"\"\"\n",
    "print(flowchart)\n",
    "\n",
    "print(\"\\n\\n⚠️ 注意すべきポイント\")\n",
    "print(\"=\"*50)\n",
    "warnings_list = [\n",
    "    \"1. 補完は情報を『創造』するのではなく『推定』する\",\n",
    "    \"2. 過度な補完は過学習の原因となる\",\n",
    "    \"3. 補完結果は必ず検証・妥当性確認を行う\",\n",
    "    \"4. ビジネス要件に反する補完は避ける\",\n",
    "    \"5. 補完前のデータも保持し、トレーサビリティを確保\"\n",
    "]\n",
    "\n",
    "for warning in warnings_list:\n",
    "    print(f\"  {warning}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 次のステップ\n",
    "\n",
    "### 学習の発展方向\n",
    "- **高度な機械学習手法**: Deep Learning based imputation\n",
    "- **時系列の特殊処理**: ARIMA, Prophet, LSTMベース補完\n",
    "- **大規模データ処理**: Dask, Apache Spark\n",
    "- **自動化パイプライン**: MLOpsでの前処理自動化\n",
    "\n",
    "### 実践課題\n",
    "1. Kaggleコンペティションデータでの前処理実践\n",
    "2. 自社データでの欠損値パターン分析\n",
    "3. A/Bテストでの補完手法効果検証\n",
    "4. リアルタイムデータストリームでの欠損値処理"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
